{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a compute environment for the pipeline steps the compute will require a Python environment with the necessary package dependencies installed, so you'll need to create a run configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call the pipeline endpoint\n",
    "\n",
    "To use the endpoint, client applications need to make a REST call over HTTP. This request must be authenticated, so an authorization header is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "import requests\n",
    "\n",
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "auth_header = interactive_auth.get_authentication_header()\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "response = requests.post(rest_endpoint, headers=auth_header, json={\"ExperimentName\": 'Run_pipeline'})\n",
    "run_id = response.json()[\"Id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.run import PipelineRun\n",
    "\n",
    "published_pipeline_run = PipelineRun(ws.experiments[experiment_name], run_id)\n",
    "pipeline_run.wait_for_completion(show_output=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schedule the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
    "\n",
    "# Submit the Pipeline every Monday at 00:00 UTC\n",
    "recurrence = ScheduleRecurrence(frequency=\"Week\", interval=1, week_days=[\"Monday\"], time_of_day=\"00:00\")\n",
    "weekly_schedule = Schedule.create(ws, name=\"weekly-diabetes-training\", \n",
    "                                  description=\"Based on time\",\n",
    "                                  pipeline_id=published_pipeline.id, \n",
    "                                  experiment_name=experiment_name, \n",
    "                                  recurrence=recurrence)\n",
    "schedules = Schedule.list(ws)\n",
    "pipeline_experiment = ws.experiments.get(experiment_name)\n",
    "latest_run = list(pipeline_experiment.get_runs())[0]\n",
    "latest_run.get_details()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a real-time inferencing service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We'll deploy the container a service named diabetes-service. The deployment process includes the following steps:\n",
    "\n",
    "    Define an inference configuration, which includes the scoring and environment files required to load and use the model.\n",
    "    Define a deployment configuration that defines the execution environment in which the service will be hosted. In this case, an Azure Container Instance.\n",
    "    Deploy the model as a web service.\n",
    "    Verify the status of the deployed service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(runtime= \"python\",entry_script=script_file,conda_file=env_file)\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n",
    "\n",
    "service_name = \"diabetes-service\"\n",
    "service = Model.deploy(ws, service_name, [model], inference_config, deployment_config)\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)\n",
    "print(service.get_logs())\n",
    "[print(webservice_name) for webservice_name in ws.webservices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Batch Inferencing Service\n",
    "\n",
    "Generate and upload batch data¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default data store\n",
    "# Enumerate all datastores, indicating which is the default\n",
    "# Load the diabetes data\n",
    "# Get a 100-item sample of the feature columns (not the diabetic label)\n",
    "# Create a folder\n",
    "# Save each sample as a separate file\n",
    "\n",
    "# Upload the files to the default datastore\n",
    "default_ds.upload(src_dir=\"batch-data\", target_path=\"batch-data\", overwrite=True, show_progress=True)\n",
    "\n",
    "# Register a dataset for the input data\n",
    "batch_data_set = Dataset.File.from_files(path=(default_ds, 'batch-data/'), validate=False)\n",
    " batch_data_set = batch_data_set.register(workspace=ws,  name='batch-data',\n",
    "                                         description='batch data',create_new_version=True)\n",
    "#Create compute¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a pipeline for batch inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "%%writefile $experiment_folder/batch_diabetes.py\n",
    "import os\n",
    "import numpy as np\n",
    "from azureml.core import Model\n",
    "import joblib\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model# Runs when the pipeline step is initialized\n",
    "\n",
    "    # load the model\n",
    "    model_path = Model.get_model_path('diabetes_model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(mini_batch):\n",
    "    resultList = []# This runs for each batch\n",
    "    # process each file in the batch\n",
    "    for f in mini_batch:\n",
    "        data = np.genfromtxt(f, delimiter=',') # Read the comma-delimited data into an array       \n",
    "        prediction = model.predict(data.reshape(1, -1))# Reshape into a 2-dimensional array for prediction (model expects multiple items)\n",
    "        resultList.append(\"{}: {}\".format(os.path.basename(f), prediction[0]))\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You're going to use a pipeline to run the batch prediction script, generate predictions from the input data, and save the results as a text file in the output folder. To do this, you can use a ParallelRunStep, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "output_dir = PipelineData(name='inferences', \n",
    "                          datastore=default_ds, \n",
    "                          output_path_on_compute='diabetes/results')\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=experiment_folder,\n",
    "    entry_script=\"batch_diabetes.py\",\n",
    "    mini_batch_size=\"5\",\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    environment=batch_env,\n",
    "    compute_target=inference_cluster,\n",
    "    node_count=2)\n",
    "\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name='batch-score-diabetes',\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[batch_data_set.as_named_input('diabetes_batch')],\n",
    "    output=output_dir,\n",
    "    arguments=[],\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\n",
    "pipeline_run = Experiment(ws, 'batch_prediction_pipeline').submit(pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "# Remove the local results folder if left over from a previous run\n",
    "shutil.rmtree('diabetes-results', ignore_errors=True)\n",
    "\n",
    "# Get the run for the first step and download its output\n",
    "prediction_run = next(pipeline_run.get_children())\n",
    "prediction_output = prediction_run.get_output_data('inferences')\n",
    "prediction_output.download(local_path='diabetes-results')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(\n",
    "    name='Diabetes_Parallel_Batch_Pipeline', description='Batch scoring of diabetes data', version='1.0')\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "auth_header = interactive_auth.get_authentication_header()\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "response = requests.post(rest_endpoint, headers=auth_header, json={\"ExperimentName\": \"Batch_Pipeline_via_REST\"})\n",
    "run_id = response.json()[\"Id\"]\n",
    "\n",
    "published_pipeline_run = PipelineRun(ws.experiments[\"Batch_Pipeline_via_REST\"], run_id)\n",
    "published_pipeline_run.wait_for_completion(show_output=True)\n",
    "\n",
    " #the results are in the output of the first pipeline step:\n",
    "# Get the run for the first step and download its output\n",
    "prediction_run = next(pipeline_run.get_children())\n",
    "prediction_output = prediction_run.get_output_data('inferences')\n",
    "prediction_output.download(local_path='diabetes-results')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enable Application Insights\n",
    "aci_service.update(enable_app_insights=True)\n",
    "endpoint = aci_service.scoring_uri\n",
    "######ceck for prediction now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the Overview page, click the link for the associated Application Insights resource.\n",
    "\n",
    "On the Application Insights blade, click Logs.\n",
    "\n",
    "    Note: If this is the first time you've opened log analytics, you may need to click Get Started to open the query editor. If a tip explaining how to write a query is displayed, close it.\n",
    "\n",
    "Paste the following query into the query editor and click Run\n",
    "\n",
    "**\n",
    " traces\n",
    " |where  message == \"STDOUT\"\n",
    "   and customDimensions.[\"Service Name\"] == \"diabetes-service-app-insights\"\n",
    " |project timestamp, customDimensions.Content\n",
    "**\n",
    "\n",
    "View the results. At first there may be none, because an ACI web service can take up to five minutes to send the telemetry to Application Insights. Wait a few minutes and re-run the query until you see the logged data and predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect and Mitigate Unfairness in Models\n",
    "you've trained a model, you can use the Fairlearn package to compare its behavior for different sensitive feature values.\n",
    "\n",
    "\n",
    "    Use the fairlearn selection_rate function to return the selection rate (percentage of positive predictions) for the overall population.\n",
    "    Use scikit-learn metric functions to calculate overall accuracy, recall, and precision metrics.\n",
    "    \n",
    "    Use a MetricFrame to calculate selection rate, accuracy, recall, and precision for each age group in the Age sensitive feature. Note that a mix of fairlearn and scikit-learn metric functions are used to calculate the performance values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import selection_rate, MetricFrame\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "# Get predictions for the witheld test data\n",
    "y_hat = diabetes_model.predict(X_test)\n",
    "overall_selection_rate = selection_rate(y_test, y_hat)\n",
    "overall_accuracy = accuracy_score(y_test, y_hat)\n",
    "overall_precision = precision_score(y_test, y_hat)\n",
    "overall_recall = recall_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'selection_rate': selection_rate,\n",
    "           'accuracy': accuracy_score,\n",
    "           'recall': recall_score,\n",
    "           'precision': precision_score}\n",
    "\n",
    "group_metrics = MetricFrame(metrics,\n",
    "                             y_test, y_hat,\n",
    "                             sensitive_features=S_test['Age'])\n",
    "\n",
    "print(group_metrics.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the dashboard visualization, which shows:\n",
    "\n",
    "    Disparity in performance - how the selected performance metric compares for the subpopulations, including underprediction (false negatives) and overprediction (false positives).\n",
    "    Disparity in predictions - A comparison of the number of positive cases per subpopulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simply removing the Age feature slightly reduces the disparity in recall, but increases the disparity in precision and accuracy. This underlines one the key difficulties in applying fairness to machine learning models - you must be clear about what fairness means in a particular context, and optimize for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.widget import FairlearnDashboard\n",
    "\n",
    "# View this model in Fairlearn's fairness dashboard, and see the disparities which appear:\n",
    "FairlearnDashboard(sensitive_features=S_test, \n",
    "                   sensitive_feature_names=['Age'],\n",
    "                   y_true=y_test,\n",
    "                   y_pred={\"diabetes_model\": diabetes_model.predict(X_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run = exp.start_logging()\n",
    "\n",
    "# Upload the dashboard to Azure Machine Learning\n",
    "try:\n",
    "    dashboard_title = \"Fairness insights of Diabetes Classifier\"\n",
    "    upload_id = upload_dashboard_dictionary(run,\n",
    "                                            dash_dict,\n",
    "                                            dashboard_name=dashboard_title)\n",
    "    print(\"\\nUploaded to id: {0}\\n\".format(upload_id))\n",
    "\n",
    "    # To test the dashboard, you can download it\n",
    "    downloaded_dict = download_dashboard_by_upload_id(run, upload_id)\n",
    "    print(downloaded_dict)\n",
    "finally:\n",
    "    run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've analyzed the model for fairness, you can use any of the mitigation techniques supported by the FairLearn package to find a model that achieves the best balance of predictive performance and fairness.\n",
    "\n",
    " you'll use the GridSearch feature, which trains multiple models in an attempt to minimize the disparity of predictive performance for the sensitive features in the dataset (in this case, the age groups). You'll optimize the models by applying the EqualizedOdds parity constraint, which tries to ensure that models that exhibit similar true and false positive rates for each sensitive feature grouping.\n",
    " \n",
    " \n",
    "    Register the models found by the GridSearch process.\n",
    "    Compute the performance and disparity metrics for the models.\n",
    "    Upload the metrics in an Azure Machine Learning experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import GridSearch, EqualizedOdds\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print('Finding mitigated models...')\n",
    "\n",
    "# Train multiple models\n",
    "sweep = GridSearch(DecisionTreeClassifier(),\n",
    "                   constraints=EqualizedOdds(),\n",
    "                   grid_size=20)\n",
    "\n",
    "sweep.fit(X_train, y_train, sensitive_features=S_train.Age)\n",
    "models = sweep.predictors_\n",
    "\n",
    "# Save the models and get predictions from them (plus the original unmitigated one for comparison)\n",
    "model_dir = 'mitigated_models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_name = 'diabetes_unmitigated'\n",
    "print(model_name)\n",
    "joblib.dump(value=diabetes_model, filename=os.path.join(model_dir, '{0}.pkl'.format(model_name)))\n",
    "predictions = {model_name: diabetes_model.predict(X_test)}\n",
    "i = 0\n",
    "for model in models:\n",
    "    i += 1\n",
    "    model_name = 'diabetes_mitigated_{0}'.format(i)\n",
    "    print(model_name)\n",
    "    joblib.dump(value=model, filename=os.path.join(model_dir, '{0}.pkl'.format(model_name)))\n",
    "    predictions[model_name] = model.predict(X_test)\n",
    "\n",
    "#Now you can use the FairLearn dashboard to compare the mitigated models:\n",
    "\n",
    "\n",
    "FairlearnDashboard(sensitive_features=S_test, \n",
    "                   sensitive_feature_names=['Age'],\n",
    "                   y_true=y_test,\n",
    "                   y_pred=predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " There are many kinds of explainer.  use a Tabular Explainer, which is a \"black box\" explainer that can be used to explain many kinds of model by invoking an appropriate SHAP model explainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.ext.blackbox import TabularExplainer\n",
    "tab_explainer = TabularExplainer(model,X_train, \n",
    "                             features=features,  classes=labels)# \"features\" and \"classes\" fields are optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluating the overall feature importance --Get global feature importance\n",
    "\n",
    "explaining individual observations --Get local feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top features by importance\n",
    "global_tab_explanation = tab_explainer.explain_global(X_train)\n",
    "global_tab_feature_importance = global_tab_explanation.get_feature_importance_dict()\n",
    "[print(feature,\":\", importance) for feature, importance in global_tab_feature_importance.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get local explanations # Get feature names and importance for each possible label\n",
    "local_tab_explanation = tab_explainer.explain_local(X_explain)\n",
    "local_tab_features = local_tab_explanation.get_ranked_local_names()\n",
    "local_tab_importance = local_tab_explanation.get_ranked_local_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(len(local_tab_features)):\n",
    "    for o in range(len(local_tab_features[l])):\n",
    "        feature_list = label[o]\n",
    "        total_support = 0\n",
    "        for f in range(len(feature_list)):\n",
    "            print(\"\\t\\t\", feature_list[f], ':', local_tab_importance[l][o][f])\n",
    "            total_support += local_tab_importance[l][o][f]\n",
    "        print(\"\\t\\t ----------\\n\\t\\t Total:\", total_support, \"Prediction:\", labels[predictions[o]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also click the View run details link in the Run Details widget to see the run in Azure Machine Learning studio, and view the Explanations tab. Then:\n",
    "\n",
    "    Select the Tabular Explanation explainer.\n",
    "    View the Global Importance chart, which shows the overall global feature importance.\n",
    "    View the Summary Importance chart, which shows each data point from the test data in a swarm, violin, or box plot.\n",
    "    Select an individual point to see the Local Feature Importance for the individual prediction for the selected data point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.run import Run\n",
    "\n",
    "# Import libraries for model explanation\n",
    "from azureml.interpret import ExplanationClient\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "run = Run.get_context()\n",
    "###############CODE#################\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=model, filename='outputs/diabetes.pkl')\n",
    "\n",
    "# Get explanation\n",
    "explainer = TabularExplainer(model, X_train, features=features, classes=labels)\n",
    "explanation = explainer.explain_global(X_test)\n",
    "# Get an Explanation Client and upload the explanation\n",
    "explain_client = ExplanationClient.from_run(run)\n",
    "explain_client.upload_model_explanation(explanation, comment='Tabular Explanation')\n",
    "\n",
    "# Complete the run\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the feature importance values\n",
    "\n",
    "With the experiment run completed, you can use the ExplanationClient class to retrieve the feature importance from the explanation registered for the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = CondaDependencies.create(conda_packages=['scikit-learn','pandas','pip'],\n",
    "                                    pip_packages=['azureml-defaults','azureml-interpret'])\n",
    "explain_env.python.conda_dependencies = packages\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory=experiment_folder,\n",
    "                      script='diabetes_training.py',\n",
    "                      environment=explain_env) \n",
    "\n",
    "from azureml.interpret import ExplanationClient\n",
    "\n",
    "# Get the feature explanations\n",
    "client = ExplanationClient.from_run(run)\n",
    "engineered_explanations = client.download_model_explanation()\n",
    "feature_importances = engineered_explanations.get_feature_importance_dict()\n",
    "\n",
    "[print(key, '\\t', value) for key, value in feature_importances.items()]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring Data Drift\n",
    "\n",
    "Over time, models can become less effective at predicting accurately due to changing trends in feature data. This phenomenon is known as data drift.\n",
    "\n",
    "The data drift monitor will run periodicaly or on-demand to compare the baseline dataset with the target dataset, to which new data will be added over time.To run the data drift monitor, you'll need a compute target.\n",
    "\n",
    " use a DataDriftDetector class to define the data drift monitor for your data. You can specify the features you want to monitor for data drift, the name of the compute target to be used to run the monitoring process, the frequency at which the data should be compared, the data drift threshold above which an alert should be triggered, and the latency (in hours) to allow for data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.datadrift import DataDriftDetector\n",
    "features = ['Pregnancies', 'Age', 'BMI']# set up feature list\n",
    "\n",
    "monitor = DataDriftDetector.create_from_datasets(ws, 'diabetes-drift-detector', baseline_data_set, target_data_set,\n",
    "                                                      compute_target=cluster_name, \n",
    "                                                      frequency='Week', \n",
    "                                                      feature_list=features, \n",
    "                                                      drift_threshold=.3, \n",
    "                                                      latency=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backfill the data drift monitor\n",
    "\n",
    "You have a baseline dataset and a target dataset that includes simulated weekly data collection for six weeks. You can use this to backfill the monitor so that it can analyze data drift between the original baseline and the target data.\n",
    "\n",
    "    Note This may take some time to run, as the compute target must be started to run the backfill analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "backfill = monitor.backfill( dt.datetime.now() - dt.timedelta(weeks=6), dt.datetime.now())\n",
    "RunDetails(backfill).show()\n",
    "backfill.wait_for_completion()\n",
    "\n",
    "# to examine data drift for the points in time collected in the backfill run.\n",
    "drift_metrics = backfill.get_metrics()\n",
    "[print(metric, drift_metrics[metric]) for metric in drift_metrics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the Datasets page, view Dataset monitors tab.\n",
    "Click the data drift monitor you want to view.\n",
    "Select date range over which you want to view data drift metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
